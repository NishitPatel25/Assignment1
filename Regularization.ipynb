{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd54a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "1) **Regularization in Deep Learning:**\n",
    "   Regularization is a technique used in deep learning to prevent overfitting and improve the generalization of the model. \n",
    "It involves adding a penalty term to the loss function during training, encouraging the model to learn simpler patterns and \n",
    "avoid complex representations that might lead to overfitting on the training data. Regularization is important because deep \n",
    "learning models, especially complex ones with many parameters, are prone to overfitting, where they memorize the training data \n",
    "instead of learning underlying patterns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dae8726",
   "metadata": {},
   "outputs": [],
   "source": [
    "2) **Bias-Variance Tradeoff and Regularization:**\n",
    "   The bias-variance tradeoff is a fundamental concept in machine learning. Bias refers to the error introduced by \n",
    "approximating a real problem with a simpler model, while variance refers to the model's sensitivity to small fluctuations \n",
    "in the training data. Regularization helps in addressing this tradeoff by adding a penalty for complex models, reducing \n",
    "variance and preventing overfitting. It encourages models to generalize better to unseen data by sacrificing some bias.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dbc3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "3) **L1 and L2 Regularization:**\n",
    "   - **L1 Regularization:** Also known as Lasso regularization, adds a penalty term proportional to the absolute value of the \n",
    "    weights. It encourages sparsity in the model by pushing less important features' weights to zero.\n",
    "   - **L2 Regularization:** Also known as Ridge regularization, adds a penalty term proportional to the square of the weights. \n",
    "    It penalizes large weights and encourages the model to distribute the importance of features more evenly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3965de96",
   "metadata": {},
   "outputs": [],
   "source": [
    "4) **Role of Regularization in Preventing Overfitting:**\n",
    "   Regularization helps prevent overfitting by penalizing complex models and reducing the model's reliance on noisy or \n",
    "irrelevant features. It constrains the model's capacity, making it generalize better to unseen data by focusing on important\n",
    "patterns rather than memorizing the training data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a5d08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "5) **Dropout Regularization:**\n",
    "   Dropout is a regularization technique where random units (neurons) are temporarily dropped or ignored during training. \n",
    "This helps in reducing overfitting by introducing noise and preventing co-adaptation of neurons. Dropout forces the network \n",
    "to learn more robust features and prevents it from relying too heavily on specific neurons, improving generalization. However, \n",
    "during inference, all neurons are used for predictions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ccb253",
   "metadata": {},
   "outputs": [],
   "source": [
    "6) **Early Stopping:**\n",
    "   Early stopping is a form of regularization where training is stopped when the models performance on a validation set \n",
    "starts to degrade. It prevents overfitting by monitoring the validation loss and stopping training before the model starts to \n",
    "memorize the training data excessively.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ac0d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "7) **Batch Normalization as Regularization:**\n",
    "   Batch Normalization is a technique that normalizes the inputs to each layer in a neural network by adjusting and scaling \n",
    "them. It acts as regularization by reducing internal covariate shift and stabilizing training, preventing the model from \n",
    "overfitting to small changes in the input distribution. Batch Normalization also allows for higher learning rates, which can \n",
    "further improve generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d7e41b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02beca7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f346c8b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c650102",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d91df78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d02b401",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8b1857",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e06a360",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
