{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cb03a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### 1. Explain the concept of batch normalization in the context of Artificial Neural Network\n",
    "\n",
    "Batch normalization is a technique used to improve the training of artificial neural networks. It addresses the problem \n",
    "of internal covariate shift, which refers to the change in the distribution of network activations due to the changes in \n",
    "the network parameters during training. This can slow down training and make it harder for the network to converge.\n",
    "\n",
    "The concept of batch normalization involves normalizing the inputs of each layer so that they have a mean of zero and a \n",
    "variance of one. This normalization is done for each mini-batch of data. By standardizing the inputs to each layer, batch \n",
    "normalization helps stabilize the learning process and improves the networks performance.\n",
    "\n",
    "### 2. Describe the benefits of using batch normalization during training\n",
    "\n",
    "Batch normalization offers several benefits during the training of neural networks:\n",
    "\n",
    "**1. Faster Convergence:**\n",
    "   - By reducing internal covariate shift, batch normalization allows the network to converge faster during training. \n",
    "     This means fewer training epochs are needed to achieve a good performance.\n",
    "\n",
    "**2. Higher Learning Rates:**\n",
    "   - Batch normalization allows for the use of higher learning rates, which can speed up the training process. Higher \n",
    "     learning rates are often risky because they can cause the training to become unstable, but batch normalization mitigates \n",
    "     this risk.\n",
    "\n",
    "**3. Reduced Sensitivity to Initialization:**\n",
    "   - Neural networks are sensitive to the initialization of weights. Batch normalization reduces this sensitivity, making \n",
    "     the network less dependent on the initial starting conditions.\n",
    "\n",
    "**4. Regularization Effect:**\n",
    "   - Batch normalization has a slight regularization effect, which can reduce the need for other forms of regularization \n",
    "     like dropout. It adds noise to the training process by normalizing each mini-batch differently, which helps prevent overfitting.\n",
    "\n",
    "**5. Improved Gradient Flow:**\n",
    "   - By normalizing the inputs to each layer, batch normalization helps maintain consistent gradient magnitudes, which improves the gradient flow through the network. This reduces the problem of vanishing or exploding gradients.\n",
    "\n",
    "### 3. Discuss the working principle of batch normalization, including the normalization step and the learnable parameters\n",
    "\n",
    "The working principle of batch normalization involves two main steps: normalization and scaling/shift using learnable parameters.\n",
    "\n",
    "**Normalization Step:**\n",
    "1. **Compute Mean and Variance:**\n",
    "   - For each mini-batch, calculate the mean (\\(\\mu_B\\)) and variance (\\(\\sigma_B^2\\)) of the inputs. These statistics are \n",
    "computed independently for each feature (dimension) across the mini-batch.\n",
    "    \n",
    "     where the number of examples in the mini-batch, and  represents the input feature values.\n",
    "\n",
    "2. **Normalize:**\n",
    "   - Use the computed mean and variance to normalize the inputs:\n",
    "     \n",
    "     Here, \\(\\epsilon\\) is a small constant added for numerical stability to prevent division by zero.\n",
    "\n",
    "**Scaling and Shifting:**\n",
    "1. **Learnable Parameters:**\n",
    "   - After normalization, introduce two learnable parameters for each feature: a scale parameter (\\(\\gamma\\)) and a \n",
    "    shift parameter (\\(\\beta\\)). These parameters allow the network to scale and shift the normalized values, giving \n",
    "    it the flexibility to represent a wider range of functions.\n",
    "     \\[\n",
    "     y_i = \\gamma \\hat{x}_i + \\beta\n",
    "     \\]\n",
    "   - These parameters are learned during the training process via backpropagation, just like the networks weights.\n",
    "\n",
    "**Overall Formula:**\n",
    "The complete batch normalization transformation can be expressed as:\n",
    "\\[\n",
    "y_i = \\gamma \\left( \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}} \\right) + \\beta\n",
    "\\]\n",
    "\n",
    "**During Inference:**\n",
    "- During inference (testing), the batch statistics (mean and variance) are not computed from the mini-batch but are \n",
    "instead estimated from a running average of the statistics computed during training. This ensures consistency in the \n",
    "predictions.\n",
    "\n",
    "### Summary\n",
    "Batch normalization is a powerful technique that stabilizes and accelerates the training of neural networks by normalizing \n",
    "the inputs of each layer. It offers several benefits, including faster convergence, higher learning rates, reduced sensitivity \n",
    "to initialization, a regularization effect, and improved gradient flow. The process involves normalizing the inputs based on \n",
    "the mean and variance of the mini-batch and then applying a learnable scaling and shifting transformation to allow the network\n",
    "to maintain its representational power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e29c370",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3e6f82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1376de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1266d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 60000\n",
      "Test dataset size: 10000\n",
      "Image batch shape: (64, 28, 28, 1)\n",
      "Label batch shape: (64, 10)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Preprocess the data\n",
    "train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "train_images = train_images.astype('float32') / 255  # Normalize pixel values to [0, 1]\n",
    "\n",
    "test_images = test_images.reshape((10000, 28, 28, 1))\n",
    "test_images = test_images.astype('float32') / 255  # Normalize pixel values to [0, 1]\n",
    "\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)\n",
    "\n",
    "# Create TensorFlow datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
    "train_dataset = train_dataset.shuffle(60000).batch(64)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels))\n",
    "test_dataset = test_dataset.batch(64)\n",
    "\n",
    "# Print the size of training and test datasets\n",
    "print(f'Training dataset size: {len(train_images)}')\n",
    "print(f'Test dataset size: {len(test_images)}')\n",
    "\n",
    "# Example of getting a batch of training data\n",
    "data_iter = iter(train_dataset)\n",
    "images, labels = data_iter.next()\n",
    "print(f'Image batch shape: {images.shape}')\n",
    "print(f'Label batch shape: {labels.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ebce30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd3f696",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6988c95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52576a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa24520",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff909a66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938fd979",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20ec672",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e354c352",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0c979c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6166c9c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcdefa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7bddcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8b6c4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a360cf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d19dea8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b073c97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c712fc32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
